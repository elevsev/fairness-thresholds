{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-One-Out Analysis\n",
    "## Purpose:\n",
    "\n",
    "* To assess the contribution and impact of each feature or data point.\n",
    "* To identify features that significantly influence model performance or fairness.\n",
    "\n",
    "## Process:\n",
    "\n",
    "1. Step 1: Train the model on the entire dataset.\n",
    "2. Step 2: For each feature or data point:\n",
    "- Exclude that feature or data point from the dataset.\n",
    "- Retrain the model on the reduced dataset.\n",
    "- Measure the change in performance or fairness metrics.\n",
    "3. Step 3: Compare the metrics with and without the excluded feature or data point to evaluate its impact.\n",
    "\n",
    "## Applications:\n",
    "\n",
    "Feature Importance: Determine which features are most influential in the model's predictions.\n",
    "Fairness Analysis: Assess the impact of excluding each feature on fairness metrics to identify features that contribute to bias.\n",
    "Model Validation: Validate the robustness and stability of the model by examining how sensitive it is to the removal of individual features or data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Results\n",
    "- Feature Importance: The calculated importance for each feature indicates how much the model's accuracy drops when that feature is excluded. Higher values suggest greater importance.\n",
    "- Fairness Analysis: In the context of fairness, similar logic can be applied to see how fairness metrics change when each feature is excluded. This helps identify features that contribute to bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Dummy data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 5)\n",
    "y = np.random.randint(0, 2, size=100)\n",
    "\n",
    "# Train initial model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "initial_accuracy = accuracy_score(y, model.predict(X))\n",
    "print(f'Initial Accuracy: {initial_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0 importance: 0.020000000000000018\n",
      "Feature 1 importance: -0.010000000000000009\n",
      "Feature 2 importance: 0.010000000000000009\n",
      "Feature 3 importance: 0.04999999999999993\n",
      "Feature 4 importance: 0.010000000000000009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.020000000000000018,\n",
       " -0.010000000000000009,\n",
       " 0.010000000000000009,\n",
       " 0.04999999999999993,\n",
       " 0.010000000000000009]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to perform leave-one-out analysis\n",
    "def leave_one_out_analysis(model, X, y):\n",
    "    feature_importances = []\n",
    "    initial_accuracy = accuracy_score(y, model.predict(X))\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        # Exclude one feature\n",
    "        X_reduced = np.delete(X, i, axis=1)\n",
    "        \n",
    "        # Retrain model on reduced dataset\n",
    "        model.fit(X_reduced, y)\n",
    "        reduced_accuracy = accuracy_score(y, model.predict(X_reduced))\n",
    "        \n",
    "        # Calculate importance\n",
    "        importance = initial_accuracy - reduced_accuracy\n",
    "        feature_importances.append(importance)\n",
    "        print(f'Feature {i} importance: {importance}')\n",
    "\n",
    "    return feature_importances\n",
    "\n",
    "# Perform leave-one-out analysis\n",
    "leave_one_out_analysis(model, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - fairness decision: \n",
    "## Key Variables\n",
    "- loo_fair_value: Fairness value obtained by excluding one feature.\n",
    "- baseline_fair_values_j: Baseline fairness value for the j-th feature.\n",
    "- n: A reference value, 0 for cases where the metric is \"difference\" and 1 for \"ratio\"\n",
    "- deltas_perf: Change in performance metric when excluding a feature.\n",
    "- PerformanceMetrics.map_perf_metric_to_group: A mapping of performance metrics to their types (e.g., regression or classification).\n",
    "- use_case_object.perf_metric_name: The name of the performance metric used in the use case.\n",
    "- suggestion: The final suggestion based on the comparison (whether to include, exclude, or examine further).\n",
    "- delta_conclusion: A string that accumulates conclusions based on the comparisons.\n",
    "\n",
    "## Decision Logic\n",
    "Comparing LOO Fairness Values with Baseline:\n",
    "\n",
    "- If leaving out feature improves fairness, exlude it. If the absolute difference between the LOO fairness value and n is less than the absolute difference between the baseline fairness value and n, it indicates that excluding the feature improves fairness.\n",
    "- Conversely, if the absolute difference between the LOO fairness value and n is greater, it indicates that excluding the feature worsens fairness - keep it.\n",
    "- If the differences are equal, further examination is needed based on performance.\n",
    "### Handling Regression and Classification Differently:\n",
    "\n",
    "* For regression:\n",
    "    * If excluding the feature does not worsen performance (deltas_perf <= 0), suggest 'exclude'.\n",
    "    * Otherwise, suggest 'examine further'.\n",
    "* For classification:\n",
    "    * If excluding the feature improves performance (deltas_perf >= 0), suggest 'exclude'.\n",
    "    * Otherwise, suggest 'examine further'.\n",
    "* Annotations for Conclusions:\n",
    "    * Depending on whether excluding the feature improves or worsens fairness, append \" (+)\" or \" (-)\" to delta_conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - tradeoff print out\n",
    "## Initialization:\n",
    "\n",
    "- i: A counter initialized to 1, used for numbering the output sections.\n",
    "- p_var: A list of protected variables from the model parameters.\n",
    "### Iterate Over Protected Variables:\n",
    "\n",
    "- For each protected variable, format and print the results of the tradeoff analysis.\n",
    "- Formatting Titles and Separators:\n",
    "\n",
    "    - Create a title string for each protected variable.\n",
    "    - Calculate a separator line to center the title within a line of 72 characters.\n",
    "    - Print the formatted title and separator line.\n",
    "### Print Trade-Off Analysis - Print sections for different threshold strategies:\n",
    "- Single Threshold: Applies the same threshold to both privileged and unprivileged groups.\n",
    "- Separated Thresholds: Applies different thresholds for privileged and unprivileged groups.\n",
    "- Separated Thresholds under Neutral Fairness: Applies different thresholds with a tolerance for neutral fairness.\n",
    "- Details of Each Section:\n",
    "\n",
    "    - For each section, print the threshold values and the best performance metric value. Use the decimal_pts attribute to format the numeric output.\n",
    "    - Print Additional Information:\n",
    "\n",
    "    - If a replacement fairness metric is used, print a note indicating the replacement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tradeoff.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "- The TradeoffRate class computes the trade-off between performance and fairness metrics for a machine learning model. This analysis is crucial for understanding how different threshold settings impact the balance between model accuracy and fairness.\n",
    "\n",
    "## Initialization\n",
    "- The __init__ method initializes the class with several attributes, many of which are derived from the usecase_obj (an instance of a use case class).\n",
    "\n",
    "## Key Attributes:\n",
    "\n",
    "- perf_metric_name: Name of the performance metric used.\n",
    "- fair_metric_name: Name of the fairness metric used.\n",
    "- metric_group: Type of the fairness metric.\n",
    "- p_var: List of protected variables.\n",
    "- result: Dictionary to store computation results.\n",
    "- fair_neutral_tolerance: Tolerance level for fairness-neutral trade-off. (It defines a range or threshold within which fairness metrics are considered to be \"neutral\" or acceptable. This tolerance is used to filter or constrain the results of trade-off analyses between performance and fairness metrics.)\n",
    "- feature_mask: Masks for protected variables.\n",
    "- map_metric_to_method: Mapping of metric names to their computation functions.\n",
    "- sigma: Standard deviation for Gaussian smoothing.\n",
    "\n",
    "## Methods\n",
    "- compute_tradeoff(self, n_threads, tdff_pbar):\n",
    "- This method computes the trade-off values and stores the results in the result attribute.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. Determine mesh grids for thresholds.\n",
    "2. For each protected variable, compute fairness and performance metrics.\n",
    "3. Apply Gaussian smoothing to the fairness metrics.\n",
    "4. Compute and store the best thresholds and associated performance metrics.\n",
    "- _compute_max_perf(self, perf_grid, fair_grid):\n",
    "5. Computes the best performance points based on different threshold settings:\n",
    "\n",
    "- Single performance metric.\n",
    "- Split performance metric.\n",
    "- Fairness-constrained performance metric.\n",
    "\n",
    "### Metric Computation Methods:\n",
    "- These methods compute specific performance and fairness metrics on a grid. Examples include:\n",
    "\n",
    "    - Balanced Accuracy: _compute_bal_accuracy_grid\n",
    "    - F1 Score: _compute_f1_grid\n",
    "    - Equal Opportunity: _compute_equal_opportunity_tr\n",
    "    - Disparate Impact: _compute_disparate_impact_tr\n",
    "    - Other Fairness Metrics: Various methods like _compute_demographic_parity_tr, _compute_false_discovery_rate_parity_tr, etc.\n",
    "### Helper Methods:\n",
    "- _compute_emp_lift_tr: Computes the empirical lift.\n",
    "- _compute_expected_profit_tr: Computes the expected profit.\n",
    "- _compute_rejected_harm_tr: Computes the harm from rejection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *gausian_smoothing*\n",
    "- A bell-shaped curve that assigns higher weights to values near the center and lower weights to values further away. \n",
    "- This process effectively averages the data points, weighted by their distance from the center, resulting in a smoother dataset.\n",
    "### Compute Fairness Values:\n",
    "\n",
    "fair_values are computed based on the fairness metric selected.\n",
    "- Apply Gaussian Smoothing:\n",
    "\n",
    "    - gaussian_filter(fair_values, self.sigma) applies Gaussian smoothing to fair_values.\n",
    "    - self.sigma is the standard deviation for the Gaussian kernel, controlling the extent of smoothing.\n",
    "    - The result, fair_values_smooted, is a smoother version of fair_values, with reduced noise and finer details.\n",
    "### Store Smoothed Fairness Values:\n",
    "The smoothed fairness values are stored in the result dictionary for further analysis and decision-making.\n",
    "\n",
    "### Why Gaussian Smoothing?\n",
    "- Noise Reduction: Smoothing helps in reducing noise in the fairness metric values, which can be due to small fluctuations in the data or model predictions.\n",
    "- Better Visualization: A smoother representation of fairness metrics makes it easier to visualize and interpret the trade-offs between performance and fairness.\n",
    "- Improved Decision-Making: Smoothing can help in identifying more stable and reliable threshold settings by minimizing the impact of minor variations.\n",
    "    - Imagine fairness metric values are plotted over different threshold settings. Without smoothing, the plot might look jagged and noisy, making it difficult to identify the optimal thresholds. Applying Gaussian smoothing would result in a smoother curve, highlighting the overall trend and making it easier to pinpoint the best threshold settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `_compute_fairness_metric_threshold` in the `CreditScoring` class is designed to compute a fairness metric threshold based on the `fair_threshold` attribute. This threshold is used to evaluate whether the model's predictions meet the specified fairness criteria.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "#### Method Definition\n",
    "\n",
    "```python\n",
    "def _compute_fairness_metric_threshold(self, priv_m_v):\n",
    "    \"\"\"\n",
    "    Computes the fairness metric threshold based on the fair_threshold variable\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    priv_m_v : float\n",
    "            Privileged metric value\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    fair_threshold : float\n",
    "            Fairness metric threshold\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "- **Purpose**: Computes the fairness metric threshold using the `fair_threshold` attribute.\n",
    "- **Parameter**:\n",
    "  - `priv_m_v`: The privileged metric value, which is a float.\n",
    "- **Returns**:\n",
    "  - `fair_threshold`: The computed fairness metric threshold, also a float.\n",
    "\n",
    "#### Handling Different Variations of Threshold Value\n",
    "\n",
    "```python\n",
    "if self.fair_threshold > 1:\n",
    "    self.fair_threshold = floor(self.fair_threshold)\n",
    "```\n",
    "\n",
    "- **Condition**: Checks if `fair_threshold` is greater than 1. This handles cases where `fair_threshold` might be provided as an integer percentage (e.g., 80 for 80%).\n",
    "- **Action**: Uses `floor` to ensure `fair_threshold` is an integer. This effectively rounds down any decimal value to the nearest whole number.\n",
    "\n",
    "#### Calculating the Fairness Metric Threshold\n",
    "\n",
    "```python\n",
    "if FairnessMetrics.map_fair_metric_to_group.get(self.fair_metric_name)[2] == 'ratio':\n",
    "    fair_threshold = 1 - (self.fair_threshold / 100)\n",
    "elif FairnessMetrics.map_fair_metric_to_group.get(self.fair_metric_name)[2] == 'difference':\n",
    "    fair_threshold = (1 - (self.fair_threshold / 100)) * priv_m_v\n",
    "\n",
    "return fair_threshold\n",
    "```\n",
    "\n",
    "- **Retrieving Metric Type**: The method looks up the type of the fairness metric (ratio or difference) from the `FairnessMetrics.map_fair_metric_to_group` dictionary.\n",
    "- **Calculating Threshold**:\n",
    "  - **For Ratio Metrics**: \n",
    "    - Computes the threshold as \\(1 - \\frac{\\text{fair\\_threshold}}{100}\\).\n",
    "    - Example: If `fair_threshold` is 80, the computed threshold will be \\(1 - \\frac{80}{100} = 0.2\\).\n",
    "  - **For Difference Metrics**:\n",
    "    - Computes the threshold as \\((1 - \\frac{\\text{fair\\_threshold}}{100}) \\times \\text{priv\\_m\\_v}\\).\n",
    "    - Example: If `fair_threshold` is 80 and `priv_m_v` is 0.5, the computed threshold will be \\((1 - \\frac{80}{100}) \\times 0.5 = 0.1\\).\n",
    "\n",
    "#### Handling Small Threshold Values\n",
    "\n",
    "```python\n",
    "else:\n",
    "    return self.fair_threshold\n",
    "```\n",
    "\n",
    "- **Condition**: If `fair_threshold` is 1 or less, it is assumed to be already in a proper format (i.e., a float between 0 and 1).\n",
    "- **Action**: Returns `fair_threshold` as is.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The `_compute_fairness_metric_threshold` method calculates the threshold value for evaluating fairness based on the `fair_threshold` attribute and the type of fairness metric (ratio or difference). For ratio metrics, the threshold is computed as \\(1 - \\frac{\\text{fair\\_threshold}}{100}\\). For difference metrics, it is computed as \\((1 - \\frac{\\text{fair\\_threshold}}{100}) \\times \\text{priv\\_m\\_v}\\). If the `fair_threshold` is already a float between 0 and 1, it is returned directly. This method ensures that the fairness evaluation is consistent with the specified threshold criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_fairness_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
